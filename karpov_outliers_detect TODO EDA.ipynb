{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы обнаружения выбросов\n",
    "\n",
    "*Подготовил Ян Пиле*\n",
    "\n",
    "Что такое выброс?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1000/1*O3lOgPwuHP7Vfc1T6NDRrQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно выбросами называют такие точки в ваших данных, которые в некотором смысле далеки от всех остальных. Говоря чуть более формально, выбросы - такие наблюдения, которые \"не вписываются\" в общее распределение данных или находятся близко к краям этого распределения. Уверен, все ввидели подобные картинки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Откуда берутся выбросы?\n",
    "\n",
    "\n",
    "### Человеческие ошибки \n",
    "\n",
    "Обычно это ошибки ввода данных. Часто такие ошибки можно встретить, например, в банковских данных. Представьте, что работник отделения банка заполняет данные о новом клиенте в CRM и, скажем, при заполнении поля \"Возраст\" случайно проставляет дополнительный ноль. Так могут \"появиться\" клиенты с возрастом > 200 лет.\n",
    "\n",
    "### Инструментальные ошибки\n",
    "Если вы собираетесь прогнозировать временной ряд изменений температуры, используя для этого поминутные показания, снятые с [термопары](https://ru.wikipedia.org/wiki/Термопара), закрепленной на улице, но при этом контакты термопары со временем окисляются (а это вносит ошибку в измерение), можно получить незапланированные отклонения \n",
    "\n",
    "\n",
    "### Ошибки проведения эксперимента \n",
    "Представим, что вы решили собрать обучающую выборку для модели преобразования речи в текст и для этого в течение всего дня записывали аудиофрагменты речи членов своей семьи. В конце концов вы решили записать свою речь и тут ваш сосед сверху начал сверлить стену перфоратором. На аудиозаписи возникнет набор частот, значительно отличающийся от стандартной человеческой речи. Более того, звуки перфоратора скорее всего заглушат вашу речь. Почему это ошибка проведения эксперимента? Вы не удостоверились, что не будет источника паразитного шума.\n",
    "\n",
    "### Ошибки обработки данных\n",
    "Представим, что вы DS, за которого обучающую выборку собирает ваш коллега, специально обученный человек (я такого не видел). Пусть данные собираются последовательностью толстых SQL-запросов с большим количество join'ов. В ходе одного из таких join'ов ваш коллега пишет:\n",
    "\n",
    "    select a.*, coalesce(b.income, -9999) as income\n",
    "    from left_table as a \n",
    "    left join right_table as b \n",
    "    on a.user_id = b.user_id\n",
    "    \n",
    "Если по кому-то из пользователей в right_table доход отсутствует, в ваших результирующих данных образуются пользователи с отрицательным доходом равным -9999. С точки зрения распределения доходов отрицательное число будет выглядеть достаточно странным. \n",
    "    \n",
    "### Ошибки сэмплирования\n",
    "Вы почти всегда пытаетесь оцнивать параметры генеральной совокупности по выборке. Может так случиться, что при формировании выборка сформировалась не вполне отражающей характеристики генеральной совокупности. Например, если у вас есть мешок с 5 черными, 8 зелеными, 5 белыми и 2 синими шариками и вы хотите получить выборку длины 20 с возвращением, с вероятностью $10^-20$ вы получите выборку из 20 синих шариков. Случай, конечно, утрированный, но суть ясна.\n",
    "\n",
    "### Явления, не описывающиеся вашей теорией\n",
    "Поиск таких отклонений - важная работа во многих науках. Это наблюдение явлений, которые на данный момент не имеют объяснения, но и не являются ошибкой. Например такое можно встретить в обработке данных с Большого Адронного Коллайдера (поиск новой физики).  Например в в 19 веке Урбен Леверье [измерил](https://ru.wikipedia.org/wiki/Смещение_перигелия_Меркурия) смещение [перигелия](https://ru.wikipedia.org/wiki/Перигелий) Меркурия и обнаружил значимое смещение в 43 угловые минуты за столетие. Это стало одним из толчков к созданию Общей Теории Относительности. (Надеюсь, что в этот момент вы не настроились за каждым выбросом в данных о выручке искать инопланетян) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Где возникают проблемы из-за выбросов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистические тесты для сравнения величин, базирующихся на суммах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы собрались в двух выборках сравнивать средние значения (как оценки математического ожидания) или выборочные дисперсии, и в наблюдениях имеют место выбросы, готовьтесь получить не то, что вы хотите."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = stats.norm.rvs(loc=10, scale=10, size=(3000,3000))\n",
    "X2 = stats.norm.rvs(loc=10, scale=2, size=(3000,3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(X2.mean(axis=0) - X1.mean(axis=0),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(X2.mean(axis=0) - X1.mean(axis=0), bins=50)\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.median(X2,axis=0) - np.median(X1,axis=0)); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = stats.norm.rvs(loc=10, scale=2, size=(900,1000))\n",
    "X2 = stats.norm.rvs(loc=10, scale=2, size=(896,1000))\n",
    "X2 = np.vstack((X2,2000*np.ones((4, 1000))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Средние теперь отличаются. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(X2.mean(axis=0) - X1.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(np.median(X2,axis=0) - np.median(X1,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели, прогнозирующие величину в смысле среднего арифметического \n",
    "### (например линейная регрессия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/abjidge/The-Complete-Guide-to-Linear-Regression-Analysis-with-Business-Problem/master/Advertising.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df вида', df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize = (12,9))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax[i,j].set_ylabel('Sales')\n",
    "        \n",
    "ax[0,0].scatter(x= df['TV'],y=df['sales'])\n",
    "ax[0,1].scatter(x= df['radio'],y=df['sales'])\n",
    "ax[1,0].scatter(x= df['newspaper'],y=df['sales'])\n",
    "ax[0,0].set_xlabel('TV Advertising')\n",
    "ax[0,1].set_xlabel('radio Advertising')\n",
    "ax[1,0].set_xlabel('newspaper Advertising')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О какой-нибудь линейной зависимости от признаков я бы тут говорить не стал и мы, в общем-то, не линейной регрессией тут занимаемся :) Давайте возьмем информацию о рекламе на радио и ТВ, возможно немного линеаризуем ее и построим на этих двух переменных регрессию на продажи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "df['sqrt_tv'] = df['TV'].apply(lambda x: np.sqrt(x))\n",
    "df['sqrt_radio'] = df['radio'].apply(lambda x: np.sqrt(x))\n",
    "fig, ax = plt.subplots(2,1, figsize = (8,9))\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_ylabel('Sales')\n",
    "        \n",
    "ax[0].scatter(x= df['sqrt_tv'],y=df['sales'])\n",
    "ax[1].scatter(x= df['sqrt_radio'],y=df['sales'])\n",
    "ax[0].set_xlabel('sqrt TV Advertising')\n",
    "ax[1].set_xlabel('sqrt radio Advertising')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(df[['sqrt_tv','sqrt_radio']], df['sales'])\n",
    "df['sqrt_model'] = reg.predict(df[['sqrt_tv','sqrt_radio']])\n",
    "\n",
    "reg2 = LinearRegression().fit(df[['TV','radio']], df['sales'])\n",
    "df['model'] = reg2.predict(df[['TV','radio']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я здесь не позаботился о том, чтобы отнормировать признаки, но представим, что наши признаки совершенно равнозначны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Среднеквадратичная ошибка модели с корнем',\n",
    "      np.sum((df['sales']-df['sqrt_model'])**2)/df.shape[0])\n",
    "print('Среднеквадратичная ошибка модели без корня',\n",
    "      np.sum((df['sales']-df['model'])**2)/df.shape[0],'\\n')\n",
    "\n",
    "print('Средняя абсолютная ошибка модели с корнем',\n",
    "      np.sum(np.abs(df['sales']-df['sqrt_model']))/df.shape[0])\n",
    "print('Средняя абсолютная  ошибка модели без корня',\n",
    "      np.sum(np.abs(df['sales']-df['model']))/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,26,0.5)\n",
    "y=np.arange(0,26,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=df[\"model\"], y=df[\"sales\"])\n",
    "plt.plot(x,y, c ='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=df[\"sqrt_model\"], y=df[\"sales\"])\n",
    "plt.plot(x,y, c ='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Испортим данные, добавив туда нехарактерное значение, скажем, признак TV == 2000 (при создании датасета случайно вбили два лишних нуля) при Sales = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = pd.DataFrame([{'TV': 2000, 'radio': 45.9, 'newspaper': 69.3, 'sales': 5}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/abjidge/The-Complete-Guide-to-Linear-Regression-Analysis-with-Business-Problem/master/Advertising.csv', index_col=0)\n",
    "df= df.append(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = LinearRegression().fit(df[['TV','radio']], df['sales'])\n",
    "df['model'] = reg2.predict(df[['TV','radio']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Среднеквадратичная ошибка модели без корня',\n",
    "      np.sum((df['sales']-df['model'])**2)/df.shape[0],'\\n')\n",
    "\n",
    "print('Средняя абсолютная  ошибка модели без корня',\n",
    "      np.sum(np.abs(df['sales']-df['model']))/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=df[\"model\"], y=df[\"sales\"])\n",
    "plt.plot(x,y, c ='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот все и испортилось :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Про методы\n",
    "\n",
    "Большинство методов обнаружения выбросов моделируют \"нормальные\" данные (в смысле не-выбросы), а все остальное называют выбросами. Мы разберем визуальный метод анализа ящика с усами и два связанных метода моделирования выбросов (а не нормальных данных), а заодно, позволяя себе некоторые вольности, разберемся с метрикой \"нормальности\" точки в IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Графическое обнаружение и boxplot. Откуда взялись 1.5 IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что наша точка имеет достаточно экстремальное значение признака TV и эту точку можно быстро найти и, например, убить. В этом может помочь старый-добрый ящик с усами в простонародье известный как боксплот. Там есть какая-то присказка про то, что \"за пределами усов\" точки можно назвать выбросами. Вот давайте посмотрим, насколько мы сможем отловить выбросы таким методом и заодно разберемся, откуда взялось правило про длину этих усов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1246/1*0MvBAT8zFSOt6sfEojbI-A.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот странное значение и нашлось. Обычно за этим надо следить при построении моделей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почему 1.5 IQR\n",
    "\n",
    "Вспомним нормальное распределение:\n",
    "<img src=\"https://miro.medium.com/max/1400/1*ARpoeY3MdhFImq0JXAXtRw.png\">\n",
    "\n",
    "и еще один факт о нем - $Q1 = -0.675\\sigma$ a $Q3 = 0.675\\sigma$\n",
    "\n",
    "Поехали считать, сколько процентов наблюдений будет между  $Q1 - n*IQR$ и  $Q3 + n*IQR$\n",
    "\n",
    "Если взять $n=1$, то \n",
    "\n",
    "\n",
    "$$Q1 - 1 * IQR = Q1 - 1 * (Q3 - Q1) = -0.675\\sigma - 1 * (0.675 - [-0.675])\\sigma = -0.675σ - 1 * 1.35\\sigma = -2.025\\sigma$$\n",
    "\n",
    "\n",
    "$$Q3 + 1 * IQR = Q3 + 1 * (Q3 - Q1) = 0.675\\sigma + 1 * (0.675 - [-0.675])\\sigma = 0.675σ + 1 * 1.35\\sigma = 2.025\\sigma$$\n",
    "\n",
    "Тогда почти 5% наблюдений будут названы выбросами. Кажется, что многовато, почему-то мы решили, что примерно до $3\\sigma$ наблюдения полезны.\n",
    "\n",
    "Таким перебором можно обнаружить, что для получения $3\\sigma$ надо взять $n = 1.7$, но это \"как-то несимметрично\", поэтому взяли 1.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ответ:\n",
    "\n",
    "1) Потому что мы хотели бы для нормальной выборки записывать в выбросы не слишком много объектов\n",
    "\n",
    "2) Потому что мы любим \"симметрию\" и красивые числа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот алгоритм использует идеи случайного леса для присвоения точкам \"скора\" нормальности. Представим дерево, которое на каждом шаге выбирает случайный признак и случайное значение этого признака (из интервала от минимума до максимума выбранного признака), а остальную выборку делит пополам - та часть, у которой значения признака больше, и та, у которой меньше. Так дерево можно строить рекурсивно до какой-то наперед заданной глубины или пока все не переберется. Такое дерево называется \"Изоляционным\" деревом или iTree. А теперь магия: если выброс - точка, которая \"выпадает\" из общего распределения, то она в таком дереве будет находиться выше, чем средняя \"нормальная\" точка. \n",
    "\n",
    "Иллюстрация:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*ujfv7WJH-tL1cRLxYEoicg.png\">\n",
    "\n",
    "Наше дерево как будто бы набрасывает прямые, параллельные координатным осям (нашим признакам) в пространство и тем самым делит его на сегменты. Теперь можно построить лес из таких деревьев. Тогда для каждого дерева нужно взять не весь набор точек, а его подмножество фиксированного размера. Осталось только разобраться, как измерить, что считается аномальным, а что нормальным. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формула:\n",
    "\n",
    "$$score = e^{-\\dfrac{E(h(n))}{c(n)}}$$\n",
    "\n",
    "$E(h(n))$ - усредненная по всем деревьям длина пути до точки \n",
    "\n",
    "$с(n)$ - средняя длина пути до точки (в бинарных деревьях поиска эта штука известна как \"средняя длина неудачного поиска\")\n",
    "\n",
    "$c(n) = 2H_{n-1}-\\dfrac{2(n-1)}{n}$\n",
    "\n",
    "$H_{n-1}$ - гармоническое число\n",
    "\n",
    "\n",
    "Если скор близок к 0, то точка аномальная, если близок к 1, то нормальная. Отсечку часто ставят на значении 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Обучение\n",
    "X_train = 0.2 * rng.randn(1000, 2)\n",
    "X_train = np.r_[X_train + 3, X_train]\n",
    "X_train = pd.DataFrame(X_train, columns = ['x1', 'x2'])\n",
    "\n",
    "# Нормальные наблюдения\n",
    "X_test = 0.2 * rng.randn(200, 2)\n",
    "X_test = np.r_[X_test + 3, X_test]\n",
    "X_test = pd.DataFrame(X_test, columns = ['x1', 'x2'])\n",
    "\n",
    "# Выбросы\n",
    "X_outliers = rng.uniform(low=-1, high=5, size=(50, 2))\n",
    "X_outliers = pd.DataFrame(X_outliers, columns = ['x1', 'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рисовалка\n",
    "plt.title(\"Данные\")\n",
    "\n",
    "p1 = plt.scatter(X_train.x1, X_train.x2, c='white',\n",
    "                 s=20*4, edgecolor='k')\n",
    "p2 = plt.scatter(X_test.x1, X_test.x2, c='green',\n",
    "                 s=20*4, edgecolor='k')\n",
    "p3 = plt.scatter(X_outliers.x1, X_outliers.x2, c='red',\n",
    "                s=20*4, edgecolor='k')\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.xlim((-2, 5))\n",
    "plt.ylim((-2, 5))\n",
    "plt.legend([p1, p2, p3],\n",
    "           [\"Обучающее\",\n",
    "            \"Новые наблюдения\", \"Выбросы\"],\n",
    "           loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest(max_samples=100, contamination = 0.1, random_state=rng)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outliers = X_outliers.assign(y = y_pred_outliers)\n",
    "\n",
    "plt.title(\"Отлов выбросов\")\n",
    "\n",
    "p1 = plt.scatter(X_train.x1, X_train.x2, c='white',\n",
    "                 s=20*4, edgecolor='k')\n",
    "p2 = plt.scatter(X_outliers.loc[X_outliers.y == -1, ['x1']], \n",
    "                 X_outliers.loc[X_outliers.y == -1, ['x2']], \n",
    "                 c='red', s=20*4, edgecolor='k')\n",
    "p3 = plt.scatter(X_outliers.loc[X_outliers.y == 1, ['x1']], \n",
    "                 X_outliers.loc[X_outliers.y == 1, ['x2']], \n",
    "                 c='green', s=20*4, edgecolor='k')\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.xlim((-2, 5))\n",
    "plt.ylim((-2, 5))\n",
    "plt.legend([p1, p2, p3],\n",
    "           [\"Обучающее\",\n",
    "            \"Пойманные выбросы\", \n",
    "            \"Пойманные нормальные наблюдения\"],\n",
    "           loc=\"lower right\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Читать](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) про IsolationForest в sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, любой метод можно попытаться \"докрутить\", IsolationForest доработали [тут](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)\n",
    "\n",
    "**Зачем**\n",
    "\n",
    "Если ваш набор точек имеет некоторую сложную структуру, обработать ее с помощью линий, параллельных осям координат может оказаться непросто даже для двумерного случая. Примеры разброса точек:\n",
    "\n",
    "<img src=\"https://github.com/sahandha/eif/raw/master/figures/Training.png\">\n",
    "\n",
    "Тепловая карта скора, полученного стандартным IsolationForest:\n",
    "\n",
    "<img src=\"https://github.com/sahandha/eif/raw/master/figures/scores_maps.png\">\n",
    "\n",
    "Видно, что многое перемешалось.\n",
    "\n",
    "Давайте вместо линий(гиперплоскостей), параллельных осям будем набрасывать просто случайные гиперплоскости(без условия параллельности осям).\n",
    "\n",
    "Тогда наши разбиения плоскостями примут вид:\n",
    "<img src=\"https://github.com/sahandha/eif/raw/master/figures/Ex1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import random as rn\n",
    "import eif as iso\n",
    "import seaborn as sb\n",
    "sb.set_style(style=\"whitegrid\")\n",
    "sb.set_color_codes()\n",
    "import scipy.ndimage\n",
    "from scipy.interpolate import griddata\n",
    "import numpy.ma as ma\n",
    "from numpy.random import uniform, seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем два кластера точек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [10, 0]\n",
    "cov1 = [[1, 0], [0, 1]]  \n",
    "\n",
    "mean2 = [0, 10]\n",
    "cov2 = [[1, 0], [0, 1]]\n",
    "\n",
    "Nobjs = 500          # Сколько точек брать\n",
    "np.random.seed(1)    \n",
    "\n",
    "x1, y1 = np.random.multivariate_normal(mean1, cov1, int(Nobjs/2)).T\n",
    "x2, y2 = np.random.multivariate_normal(mean2, cov2, int(Nobjs/2)).T\n",
    "\n",
    "# Собираем в один датасет\n",
    "x = np.concatenate((x1,x2))\n",
    "y = np.concatenate((y1,y2))\n",
    "X=np.array([x,y]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картиночка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(6,6))\n",
    "fig.add_subplot(111)\n",
    "plt.plot(X[:,0],X[:,1],'o', color=[0.5,0.5,0.5])\n",
    "plt.grid(\"off\")\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "plt.xlim([-5,15])\n",
    "plt.ylim([-5,15])\n",
    "plt.tick_params(direction='out', length=6, width=2, colors='k',labelsize=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним результаты от обычного леса и \"улучшенного\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F0  = iso.iForest(X,ntrees=500, sample_size=256, ExtensionLevel=0)\n",
    "F1  = iso.iForest(X,ntrees=500, sample_size=256, ExtensionLevel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скорим обучающую выборку\n",
    "S0 = F0.compute_paths(X_in=X)\n",
    "S1 = F1.compute_paths(X_in=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss0=np.argsort(S0)\n",
    "ss1=np.argsort(S1)\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x,y,s=15,c='b',edgecolor='b')\n",
    "plt.scatter(x[ss0[-10:]],y[ss0[-10:]],s=55,c='k')\n",
    "plt.scatter(x[ss0[:10]],y[ss0[:10]],s=55,c='r')\n",
    "plt.title('standard')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x,y,s=15,c='b',edgecolor='b')\n",
    "plt.scatter(x[ss1[-10:]],y[ss1[-10:]],s=55,c='k')\n",
    "plt.scatter(x[ss1[:10]],y[ss1[:10]],s=55,c='r')\n",
    "plt.title('extended')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь нарисуем тепловую карту скоров. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-5, 15, 30), np.linspace(-5, 15, 30))\n",
    "\n",
    "S0 = F0.compute_paths(X_in=np.c_[xx.ravel(), yy.ravel()])\n",
    "S0 = S0.reshape(xx.shape)\n",
    "\n",
    "S1 = F1.compute_paths(X_in=np.c_[xx.ravel(), yy.ravel()])\n",
    "S1 = S1.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,6))\n",
    "\n",
    "ax1 = f.add_subplot(121)\n",
    "levels = np.linspace(np.min(S0),np.max(S0),10)\n",
    "CS = ax1.contourf(xx, yy, S0, levels, cmap=plt.cm.YlOrRd)\n",
    "plt.scatter(x,y,s=15,c='None',edgecolor='k')\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "\n",
    "ax2 = f.add_subplot(122)\n",
    "levels = np.linspace(np.min(S1),np.max(S0),10)\n",
    "CS = ax2.contourf(xx, yy, S1, levels, cmap=plt.cm.YlOrRd)\n",
    "plt.scatter(x,y,s=15,c='None',edgecolor='k')\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А как еще бывает?\n",
    "\n",
    "Например есть библиотека PyOD. Примеры [здесь](https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1)\n",
    "\n",
    "[Одноклассовый SVM](https://towardsdatascience.com/outlier-detection-with-one-class-svms-5403a1a1878c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
