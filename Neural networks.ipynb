{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Введение в нейронные сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Это краткий конспект полезных выводов из курса по нейросетям - https://habr.com/ru/articles/414165/\n",
    "# Также - полезная статья Яндекса со ссылками на источники - https://habr.com/ru/companies/yandex/articles/307260/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Общая ML теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Задача ML = задача оптимизации весов модели на тренировочной выборке для лучшего предикта отложенной.  \n",
    "- Пусть у нас есть объекты $u_1, u_2, ...$ (например, клиенты)\n",
    "- Каждый объект $u_i$ может быть описан признаками в N-мерном пр-ве $x_{11}, x_{12}, ... x_{1N}$ \n",
    "- Мы решаем задачу классификации -> определяем какому из классов принадлежат объекты: $y_1, ... y_M$\n",
    "\n",
    "Есть тренировочный размеченный датасет Train = $u_1, u_2...$     \n",
    "Мы обучаем модель типа $y = Q(w_{1}, w_{2}, ... w_{K}, u_{1}, ... u_{N})$ с какими то весами  \n",
    "А далее делаем предсказание на отложенной выборке predict(Q, u_test) -> y_predict  \n",
    "\n",
    "\n",
    "___\n",
    "<u>Линейный классификатор</u>  \n",
    "y = (w, x) или в матричном виде: $W_{NM} * X_{N} = Y_{M}$  \n",
    "Здесь - $X_{N}$ столбец признаков объекта, $Y_{M}$ - столбец вероятностей принадлежности к каждому классу  \n",
    "\n",
    "___\n",
    "<u>Метод макс правдоподобия</u>  \n",
    "Макс. правдоподобие - общая метрика, которую надо оптимизировать для поиска корректных весов модели  \n",
    "\n",
    "Likelihood = p(y=y_train_1 | u_train_1) * ... * p(y=y_train_D | u_train_D)  \n",
    "Это вероятность того что мы получим верный $y_{train}$ из возможных $y_1, y_2 ... y_M$ для каждого объекта $u_{train}$  \n",
    "\n",
    "Вероятность получить $y_i$ для каждого u = x1, x2... xN задается через модель и ее веса **w**  \n",
    "\n",
    "Для оптимальных весов необходимо Likelihood -> MAX  \n",
    "Это эквивалентно -ln(Likelihood) -> MIN  \n",
    "Или Lossfunc = Loss $= -\\sum \\ln(p(y = y_{train} | u_{train})) -> min$ \n",
    "\n",
    "Для поиска минимума функции используется градиентный спуск\n",
    "\n",
    "___\n",
    "<u>Регуляризация</u>\n",
    "Для линейного классификатора можно написать LossFunc = L(w, x) -> grad(LossFunc, w)  \n",
    "Однако минимум Lossfunc может быть найден при разных значениях w (нет единственного решения).  \n",
    "Поэтому чтобы это исправить и норм оптимизировать - делают **регуляризацию**\n",
    "\n",
    "Lossfunc $= -\\sum \\ln(p) + \\lambda \\cdot R(w)$  \n",
    "\n",
    "Варианты R(w)  \n",
    "L1: R = |w1| + |w2| + ...  \n",
    "L2: $R^2 = w_1^2 + w_2^2 + ...$    \n",
    "\n",
    "L2 хорошо дифференциируется и помимо добавления однозначности - наказывает модель за очень большие $w_i$  \n",
    "Оказывается, плавное распределение значений весов $w_1, w_2 ...$ положительно сказывается на том, чтобы  \n",
    "модель не переобучалась (то есть не искала слишком частных решений которые фитят Train).  \n",
    "\n",
    "Итак, с помощью регуляризации получаем единственную точку для оптимизации, страхуемся от переобучения  \n",
    "lambda - фактор, задающий насколько сильно наказываем модель за избыточные веса. Подбирается как гиперпараметр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "В линейном классификаторе мы по сути делаем следующие преобразования:  \n",
    "xv = (x1, ... xN)  ; $xv * w_{NM} = y_M$  \n",
    "\n",
    "Для нейросетей используем следующие цепочки преобразований:   \n",
    "tmp = xv * w_N_K1 # на выходе получим столбец величины K1  \n",
    "tmp = L(tmp) # применяем некоторое нелинейное преобразование (функция активации)  \n",
    "tmp = tmp * w_K1_K2 # очередное \"взвешивание\" матрицей - на выходе столбец K2  \n",
    "tmp = L(tmp)  \n",
    "...  \n",
    "tmp = tmp * w_Kx_M = y_M # здесь получаем столбец вероятностей принадлежности к классам  \n",
    "\n",
    "каждое умножение на матрицу w_x_y -> это новый **слой** обучаемой нейросети  \n",
    "Функция активации L(x) нелинейная, чтобы получаемая модель Q была более гибкой и настраиваемой.  \n",
    "Один из эффективных вариантов для обучения: L = Relu(x) = x if x > 0 else 0.  \n",
    "\n",
    "\n",
    "<u>Аналогия с нейронами</u>  \n",
    "Нейрон - это элемент принимающий на вход N сигналов типа x1, ... xN и выдающий M сигналов,  \n",
    "Линейный классификатор - это простейший нейрон  \n",
    "Каждый слой нейросети - матрица типа $W_{nm}$ <u>строка</u> которой является нейроном  \n",
    "Строка принимает на вход вектор объекта x1,...xN и возвращает y1,...yM\n",
    "\n",
    "___\n",
    "Мы получаем модель с рядом слоев, каждый из которых содержит K_x * K_y весов в общем случае.  \n",
    "Так как в каждом из слоев - линейное умножение, то можно аналитически записать LikeliHood модели.  \n",
    "Далее, также как и в другой модели - считаем $\\nabla Loss$, смещаем веса в направлении dw = $-\\nabla Loss$ и двигаемся  \n",
    "иттеративно пока не найдем минимум функции. Данные веса w_final - веса обученной нейросети\n",
    "\n",
    "При этом расчет градиентов на каждом шаге сводится к матричным операциям со слоями и хорошо параллелится  \n",
    "в плане вычислений. Поэтому обучение сетей производится на видеокартах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Сверточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Проблема классических сетей в огромном кол-ве обучаемых параметров  \n",
    "Каждый слой - это по сути матрица которая взвешивает каждый элемент  \n",
    "У одного объекта обучающей выборки u может быть широкое признаковое пространство N.   \n",
    "Например, если это картинка - то в ней может быть N>100k (rgb * число пикселей).  \n",
    "\n",
    "Принцип сверточных сетей - когда в каждом слое взвешивание (свертка) с весами идет по принципу  \n",
    "один нейрон слоя = некоторая область предудущего слоя (или объекта) - но не весь.  \n",
    "\n",
    "<img src=\"images/cnn.png\" width=\"500\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Промежуточные слои сверточной сети - это новые признаки вместо исходных признаков объектов.  \n",
    "Их можно рассматривать как более высокие уровни абстрации, генерируемые моделью в процессе обучения  \n",
    "так, чтобы уже на них осуществлять более эффективную классификацию.  \n",
    "\"Подсматривая\" в эти признаки, можно оценивать какие детали наиболее важны для модели\n",
    "\n",
    "Модель состоит из слоев типа convolution (свертка с обучаемым ядром), pooling (изменение размерности слоя),\n",
    "fully connected (слой, восстанавливающий размерность по числу классов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Текстовые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<u> Общий пайплайн </u>\n",
    "1. Input (word vector)  \n",
    "2. Морфология (преобразования слов, падежи, окончания итд)\n",
    "3. Синтаксис (сборка слов в предложения, учет правил)\n",
    "4. Семантика (оценка \"настроения\" предложения, посыл итд)\n",
    "5. Контекст (перенос смыслов между несколькими предложениями)\n",
    "\n",
    "Имеем некоторый язык со всеми его словами (N ~ несколько млн)   \n",
    "Каждое слово здесь w = (0, 0, ... 1, 0, ... 0) - огромный вектор  \n",
    "\n",
    "Вектор слова w -> нейросеть -> не дискретный вектор wn = (0.1, -2, 3.2, ...)  \n",
    "Вектора wn - это абстрактное представление модели, в которое однозначно переводится вектор w  \n",
    "Вектор wn может быть меньшего размера, а также обладает свойствами семантической близости\n",
    "\n",
    "___\n",
    "<u> Обучение языковой модели - word2vec </u>  \n",
    "База - предсказание слова по входному контексту.  \n",
    "Мы даем модели на вход слова w1, w2, w3 ... wk -> она генерит вероятность встретить след. слово w_k+1  \n",
    "Можно например взять большой объем текста (все статьи Wiki) и идти по ним скользящим окном k, формируя Train  \n",
    "Точнее будет также брать слова до/после пропуска: \"Старик поймал __ рыбку\" (окно контекста)   \n",
    "В итоге для каждых двух слов мы можем оценить некоторую вероятность их совместного соседства.  \n",
    "Можно обучить модель так, чтобы итоговые вектора каждого слова были в таком пространстве, что  \n",
    "скалярное произведение этих векторов будет давать как раз вероятность соседства = близость слов.  \n",
    "При этом для модели - положительный факт встречи слов = текста, отрицательный = рандомные группы слов.  \n",
    "Неплохая статья - https://habr.com/ru/articles/446530/  \n",
    "Доп инфа по работе с языковыми моделями - https://web.stanford.edu/~jurafsky/slp3/3.pdf  \n",
    "Лекция по работе в w2v - https://www.youtube.com/watch?v=U0LOSHY7U5Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Пример обучения w2v модели на искусственном датасете. Для большей точности необходимо\n",
    "# импортировать датасеты с наборами слов извне\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample text data\n",
    "sentences = [\n",
    "    ['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    ['this', 'is', 'the', 'second', 'sentence'],\n",
    "    ['yet', 'another', 'sentence'],\n",
    "    ['one', 'more', 'sentence'],\n",
    "    ['and', 'the', 'final', 'sentence']\n",
    "]\n",
    "# Train Word2Vec model - \n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word_vector = model.wv['sentence']\n",
    "similar_words = model.wv.most_similar('sentence')\n",
    "print(\"Similar words to 'sentence':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Дополнительные ньюансы обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "См подробнее в лекции - https://www.youtube.com/watch?v=OJlqsLg4E9Q\n",
    "\n",
    "<u> Проблема выбора функции активации </u>  \n",
    "Если выбрать функцию в стиле сигмоиды, то можно столкнуться с vanishing gradients -- градиенты  \n",
    "при решении оптимизационной задачи быстро будут зануляться и обучение будет останавливаться.  \n",
    "Функция Relu(x) лишена таких проблем\n",
    "\n",
    "<u> Проблема инициализации весов </u>  \n",
    "При случайной инициализации исходных весов модели w есть риск повышения нестабильности с каждым новым слоем.  \n",
    "Так как градиенты обучения будут пропорциональны весам и будут усиливать тенденцию к росту / спаду параметров.  \n",
    "Вариант - нормализировать веса после каждого слоя - приводить к распределению с тренируемыми параметрами.  \n",
    "В итоге модель сама подстроится - какую лучше пост-нормализацию после каждого слоя выбирать.  \n",
    "Это откроет путь к поиску существенно более стабильных конфигураций на базе большого кол-ва слоев.  \n",
    "Техника называется Batch normalization  \n",
    "\n",
    "То есть мы треним модель сразу на группе семплов - получаем от них статистику (среднее, дисперсия)  \n",
    "Используем их для пост-нормализации с подбираемыми параметрами avg, std  \n",
    "А при использовании обученной модели (фаза predict) -- подаем на вход один семпл  \n",
    "и используем полученные за тренировку значения статистики.  \n",
    "\n",
    "<u> Проблема переобучения </u>  \n",
    "Модель не должна наделять отдельные веса из группы весов исключительными свойствами -  \n",
    "это ведет к переобучению. Пример: если модель опознает по фото кошек, что это животное с хвостом  \n",
    "и фичу типа \"хвост\" определяет как ключевую - то легко может переобучиться. Нужно сказать ей что то  \n",
    "вроде \"смотри на хвост, но и на все другие признаки в совокупности\". То есть штрафовать за чрезмерное  \n",
    "внимание только к хвосту.  \n",
    "В линейном классификаторе (см выше) решали это регуляризацией - штрафом за величину отдельных весов.  \n",
    "В нейронной сети работает концепция <u> dropout </u> -- каждое обучение случайно дропаем часть (x%)  \n",
    "нейронов из каждого слоя. Это не дает завязаться на одном нейроне и перераспределяет веса на всю группу\n",
    "\n",
    "<img src=\"images/dropout.png\" width=\"400\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<u> Проблема градиентного спуска </u>  \n",
    "Классический расчет градиента в некоторой точке M-мерной функции потерь - вычислительно долгая задача.  \n",
    "Порой используют стохастический спуск - случайный шаг в сторону -grad по любому из измерений. Больше не  \n",
    "совсем точных шагов, но зато быстрая сходимость глобально.  \n",
    "Также ускоряют алгоритмы подстраивая шаг оптимизации под крутизну спуска итд.  \n",
    "\n",
    "Когда M значительно - <u>редко</u> можно встретить локальные минимумы, но часто - седловые точки. Когда все  \n",
    "производные равны нулю, но вторые производные разных знаков. В этих точках спуск сильно буксует, прежде чем  \n",
    "найти в каком направлении спуститься с седла. Есть улучшения методов, которые ускоряют этот процесс.  \n",
    "\n",
    "Для реализации градиентного спуска в нейросетях используется метод back propagation.  \n",
    "Сначала задаем вектор весов w, далее рассчитываем на тренировочных данных значения на всех узлах сети.  \n",
    "Затем идем в обратном направлении, корректируя веса в узлах так, чтобы минимизировать отклонение от ожидаемого сигнала.   \n",
    "\n",
    "<u> Learning rate </u>  \n",
    "Эпоха (epoch) - иттерация в течение которой все данные Train пропущены через модель, определены градиенты весов w  \n",
    "Далее мы делаем изменение весов W_upd = W - learning_rate * grad(W). И идет следующая эпоха итд.  \n",
    "Величина learning_rate влияет на скорость обучения - не должна быть очень низкой или высокой.  \n",
    "Хорошая практика, когда сначала берем побольше, а каждые X эпох learning_rate уменьшается.\n",
    "\n",
    "<u> Оптимизация гиперпараметров </u>  \n",
    "Может быть много параметров (droout_percent, learning_rate, ...) которые надо подбирать для задачи.  \n",
    "Обычно выбирается 1 fold (train_date / validation_data) и на нем после train-обучения проверяется модель.  \n",
    "Можно делать grid-перебор гиперпараметров в поисках лучшего, либо random - чтобы в выборку попадало  \n",
    "больше семплов потенциально важных параметров.  \n",
    "\n",
    "Сначала перебор идет на макро масштабе, потом масштаб уточняется (по мере уточнения поиска).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Обучение с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<u> Общая идея </u>  \n",
    "Обучаемая система - некоторый агент взаимодействующий с внешней средой, который совершает последовательно   действия (a=action) и на каждое действие получает от среды подкрепление (r=reward) - полож. или отрицательное.  \n",
    "Сессия взаимодействия агента со средой длится N иттераций (s = session). Агент учится так, чтобы  \n",
    "среднее подкрепление в течение сессии было максимальным.  \n",
    "\n",
    "Пример: обучение игре в аэрохоккей (коричневый скрин справа).  \n",
    "Передаем на вход агенту разницу previous/next изображений, ограничиваем взаимодействие N=50 шагами (сессия)  \n",
    "В конце сессии, если агент выиграл (шарик не упал) - дает reward>0 всем шагам. И наборот при поражении.  \n",
    "Прогоняем обучение много раз, чтобы агент делал действия (вверх/вниз) так, чтобы максимизировать reward за N=50  \n",
    "PS. reward за шаги может быть распределен неравномерно (затухать по мере увеличения N - для стабильности)  \n",
    "\n",
    "<img src=\"images/reinforcement_learning.png\" width=\"500\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Рекуррентные сети (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Классические нейронные сети имеют только один вход (один объект u) -> и несколько выходов (Y)  \n",
    "Хочется организовать возможность передавать на вход сети последовательность объектов u1, u2, ...   \n",
    "и получать ответы исходя из суммарного контекста.  \n",
    "К примеру, в задаче машинного перевода важно учитывать входную последовательность слов и их  \n",
    "порядок для генерации выходного перевода.  \n",
    "\n",
    "Это можно решить через RNN, когда каждый слой сети на вход получает инфу от своего элемента, а также предыдущего.  \n",
    "На скрине справа сеть тренируется предсказывать следующий символ по предыдущим.  \n",
    "В output она выдает следующую букву последовательности hello world\n",
    "\n",
    "Если, к примеру, обучить модель на Шекспире - то она будет пытаться говорить следующий символ (-> слово)  \n",
    "так, чтобы максимально попадать в текста Шекспира (то есть по сути впитает \"дух\" автора).  \n",
    "Теперь новые входные текста она будет дополнять в духе Шекспира.\n",
    "\n",
    "PS. Для обучения и расчета градиентов рекурентную сеть \"разматывают\" в линейную во времени  \n",
    "Это приводит к очень длинным цепочкам и большим вычислениям, поэтому есть архитектуры вроде LSTM,  \n",
    "которая помогает облегчить вычисления\n",
    "\n",
    "<img src=\"images/rnn.png\" width=\"350\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заметки на базе лекции Глеба Кудрявцева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>LLM и ее память</u>  \n",
    "LLM = конструкция, принимающая на вход набор токенов (промпт) и отдающая набор токенов.  \n",
    "Сама LLM это нейросеть с триллионами весов, обученная генерировать приемлеый промпт на базе интернета (pre-train)  \n",
    "Дообучается с подкреплением после этого для того чтобы ответы были точнее, имели нужный окрас и пр (post-train)  \n",
    "К примеру: ее дообучают, чтобы она отделяла инструкции вида \"RULES: ...; CONTEXT: ...\" итд.  \n",
    "  \n",
    "У обычного процессора: внешняя память -> ЦПУ (вычисления)  \n",
    "Для LLM роль памяти играет контекстное окно - может быть очень большим (сотни тыс токенов)  \n",
    "  \n",
    "Помимо управляющего промпта для нейросети присутствует еще системный промпт:  \n",
    "дополнительные инструкции, которые позволяют нейросети адекватнее реагировать на разные запросы.  \n",
    "Пример - tool-calling, когда пользователь просит нарисовать картинку - нейросеть выдает текст в  \n",
    "виде json с параметрами которые уже можно использовать для прорисовки\n",
    "  \n",
    "<u>Токены</u>  \n",
    "Модель обучается на токенах = частях слов, слогах и пр. Это позволяет более гибко чем по  \n",
    "словам учитывать структуру языка и лучше улавливать его закономерности.  \n",
    "База токенов может включать различные языковые алфавиты (латинский, китайский, кирилллицу итд)  \n",
    "Разные нейросети по разному разбивают на токены входной текст промпта - можно смотреть здесь  \n",
    "https://tiktokenizer.vercel.app\n",
    "  \n",
    "<u>RAG</u>  \n",
    "Retrieval Augmented Generation = генерация с дополненной выборкой  \n",
    "Нейросеть получает промпт обогащенный контекстом интересующей базы знаний и помогает работать с ним.  \n",
    "При этом используются ключевые инструкции вроде \"отвечай только согласно данному контексту\"  \n",
    "Пример: ответы на вопросы касательно имеющейся в компании базы знаний  \n",
    "  \n",
    "Объем базы знаний может быть очень большим и не поместится в контекстное окно.  \n",
    "Для этого база знаний разбивается на небольшие куски (~1000 символов), векторизуется (эмбединги)  \n",
    "и далее в зависимости от запроса пользователя в LLM подаются семантически близкие запросу куски базы.  \n",
    "  \n",
    "---\n",
    "Далее используем инфу от Карпатого  \n",
    "https://www.youtube.com/watch?v=7xTGNNLPyMI  \n",
    "<u>Pre-train process</u>  \n",
    "Подготавливается тренировочный датасет - это слепок всего Интернета в текстовом виде.  \n",
    "Его можно взять у специализированных парсинговых компаний вроде huggingface -  \n",
    "интернет страницы парсятся, чистятся от мета информации (вроде html-тегов), контекст фильтруется  \n",
    "эвристиками, фильтруются нежелательные сайты/ресурсы. Далее мы получаем огромный монолитный кусок текста в ~40ТБ.  \n",
    "Текст токенизируется (см выше) - раскидывается в базис, в одной из таких систем предусмотрено 10 тыс токенов,  \n",
    "т е некоторый слог/сочетание = уникальный токен (id). Токенизация также чувствительна к UPPER/LOWER CASE.  \n",
    "  \n",
    "Далее скользящим образом идет движение по тексту, выбирается окно случайного размера (0-10 тыс токенов,  \n",
    "по разному) - это контекст. На базе него нейросеть пытается предсказать следующий токен - выдает вероятности.  \n",
    "Далее на базе полученного фидбека через back propagation осуществляется подгонка весов модели так, чтобы  \n",
    "увеличить score угадываний по всему датасету (пример метрики качества = perplexity)  \n",
    "  \n",
    "Архитектура сети задается слоями и нейронами (матрицами преобразования чисел) - об этом см раздел выше.  \n",
    "Сеть тренируется эпохами (epoche) - иттерация первого прохода (forward propagation) когда для входного контекста  \n",
    "на имеющихся весах вычисляется результат и обратного (back propagation) когда по вычисляемой функции потерь  \n",
    "мы проходимся назад и вычисляем градиенты для весов (куда их сместить чтобы результат стал точнее)  \n",
    "  \n",
    "Спустя N эпох функция потерь значительно снижается - веса модели подобраны под то, чтобы лучше чувствовать  \n",
    "связность текста. Теперь, если ей вбросить некоторый контекст - она будет авто-дополнять его в стиле  \n",
    "pre-train выборки (к примеру, если учить ее на Шекспире, то ответы будут в его духе)  \n",
    "  \n",
    "Однако, на этом этапе речь НЕ идет об ассистенте - это по сути продвинутый auto-completer контекста  \n",
    "При этом, автодополняющая нейросеть выучила особенности Интернета (если train set 44ТБ) - и сжала их  \n",
    "до 10-15 триллионов параметров-весов, что то вроде ZIP архивации  \n",
    "(!) возвращает к рассуждениям о том что \"интеллект - это форма архивации знаний\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Языковые модели (todo jup notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Токенизация текстов</u>  \n",
    "Имеем на входе текст - набор слов, символов. Хотим сопоставить ему вектор в некотором базисе.  \n",
    "Один из вариантов - частотная кодировка (используется в BoW) - (1, 2, ... 0) по словарю всех слов.  \n",
    "Можно использовать n-gram-ы - последовательные наборы из n слов текста - словарь сильно больше, зато  \n",
    "взаимосвязи потом считать гибче. Т. е берем весь корпус текстов - и например для биграмм (n=2)  \n",
    "вводим вектор всех пар слов которые последовательно встречаются в тексте.  \n",
    "  \n",
    "Вариант разбиения на токены - когда каждый токен = часть слова (слог или даже буква), вводится  \n",
    "ориентируясь на специфику корпуса текста чтоб эффективно его закодировать.  \n",
    "в итоге текст вида (\"word1 word2 ...\") преобразуется в (id1 id2 id3 ...) набор токенов\n",
    "\n",
    "<u>Построение эмбеддингов</u>  \n",
    "Задача: имеем похожие по смыслу слова/тексты - хотим преобразовать их в вектора фикс длины, такие что  \n",
    "чем более похожи по смыслу тексты - тем ближе cos мера данных текстов.  \n",
    "  \n",
    "**BOW** = простой бейзлайн эмбедингов без нейросети  \n",
    "BoW = bag of words. Есть вектор длиной = все используемые слова языка (неск десятков тыс например).  \n",
    "Тогда некоторое слово = (0, 0, ... 1, 0, ...) sparse вектор.  \n",
    "(!) BoW это частный случа n-gram, где n = 1  \n",
    "Текст можно представить как = (1, 2, 0, ... 5) -> нормировать и получить частотность встречания слов.  \n",
    "Не учитывается порядок слов -> близкие тексты = тексты с близкими наборами слов.  \n",
    "Близость векторов можно оценивать как cos(x,y)  \n",
    "\n",
    "**TF-IDF** = Term Frequency – Inverse Document Frequency = взвешенный вариант BOW.  \n",
    "Для каждого слова x в документе d считаем координату:   \n",
    "x = TF * IDF; TF = % слова x в документе d, IDF = инверсная доля документов (всех) в которых встречается слово x.   \n",
    "В TF-IDF каждая координата взвешивается с учётом важности слова в документе и редкости слова в корпусе, чтобы    типичные «шумные» слова имели меньшее влияние  \n",
    "К примеру, в документе много слов \"и\", \"на\" - но их и везде много - поэтому их вес понижается.  \n",
    "А вот если слова специфичные для документа и используются часто - их вес в векторе растет.  \n",
    "В итоге, x получает модифицированную частоту с учетом важности (в bow только частоту)  \n",
    "  \n",
    "**word2vec** = эмбединги через простую нейросеть  \n",
    "Идея: создаем нейросеть с одним скрытым слоем = матрицей весов W.  \n",
    "Пусть в обучающем корпусе N слов, учим нейросеть предсказывать вероятность того что для слова X  \n",
    "слова y1, y2, ... yN-1 окажутся в контекстном окне (например рядом с ним в тексте +- 3 слова)  \n",
    "Схема нейросети: входной вектор слова X (0, ... 1, ... 0) -> X * W -> emb_vec -> softmax(emb_vec)  \n",
    "emb_vec - вектор фикс длины который получается при умножении на матрицу W - эмбединг слова.  \n",
    "softmax переводит emb_vec в вектор вероятностей N-1: шанс что разные слова будут в контексте.  \n",
    "Учим сеть, оптимизируем W. После этого можно для любого слова строить эмбединг word_vec * W   \n",
    "  \n",
    "Эмбединг текста здесь = агрегация эмбединга слов, то есть он контекст-независим, тоже слабый.  \n",
    "  \n",
    "**DSSM** = трансформеры для контекстуально зависимых эмбедингов  \n",
    "Создаем размеченный датасет вида: текст X похож на текст Y итд.  \n",
    "Это может быть например \"запрос в поисковике\" - \"текст документа по которому кликнули\" итд. \n",
    "Можно заранее знать какие тексты принадлежат одному сегменту и сгруппировать их - в общем, задаем похожесть.  \n",
    "  \n",
    "Далее, нейросеть в несколько слоев обучаем давать на выходе X, Y -> transformer -> X_emb, Y_emb такие, чтобы  \n",
    "cos(X_emb, Y_emb) -> 0 для похожих документов. X, Y на входе подаем также в виде X_vec, Y_vec используя  \n",
    "например n-gram или Bow.  \n",
    "  \n",
    "Возможны разные варианты DSSM:  \n",
    "1) симметричный эмбеддинг = одна и та же сеть для X -> X_emb, Y -> Y_emb = подходит для векторизации одного  \n",
    "пространства текстов чтобы потом искать семантически похожие  \n",
    "2) ассиметричный = разные сети для X, Y если природа их пространств разная (например, запрос - текст)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Пример обучения маленькой нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3370\n",
      "Epoch 100, Loss: 0.0505\n",
      "Epoch 200, Loss: 0.0174\n",
      "Epoch 300, Loss: 0.0103\n",
      "Epoch 400, Loss: 0.0072\n",
      "Epoch 500, Loss: 0.0056\n",
      "Epoch 600, Loss: 0.0045\n",
      "Epoch 700, Loss: 0.0038\n",
      "Epoch 800, Loss: 0.0033\n",
      "Epoch 900, Loss: 0.0029\n",
      "b c d e f "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Данные ---\n",
    "tokens = ['a','b','c','d','e','f','g','h','i','j']\n",
    "token_to_idx = {t:i for i,t in enumerate(tokens)}\n",
    "idx_to_token = {i:t for i,t in enumerate(tokens)}\n",
    "\n",
    "# Пример пары контекст->следующий токен\n",
    "data = [\n",
    "    ('a','b'),\n",
    "    ('b','c'),\n",
    "    ('c','d'),\n",
    "    ('d','e'),\n",
    "    ('e','f'),\n",
    "]\n",
    "\n",
    "def one_hot(idx, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[idx] = 1\n",
    "    return vec\n",
    "\n",
    "# --- Параметры сети ---\n",
    "input_size = len(tokens)\n",
    "hidden_size = 5\n",
    "output_size = len(tokens)\n",
    "lr = 0.1  # learning rate\n",
    "\n",
    "# Случайная инициализация весов\n",
    "W1 = np.random.randn(hidden_size, input_size) * 0.1\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = np.random.randn(output_size, hidden_size) * 0.1\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# --- Функции ---\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def forward(x):\n",
    "    z1 = W1 @ x + b1\n",
    "    h = np.tanh(z1)\n",
    "    z2 = W2 @ h + b2\n",
    "    y = softmax(z2)\n",
    "    return y, h\n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    return -np.log(y_pred[y_true] + 1e-8)\n",
    "\n",
    "# --- Обучение ---\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for ctx, target in data:\n",
    "        x = one_hot(token_to_idx[ctx], input_size)\n",
    "        y_true = token_to_idx[target]\n",
    "\n",
    "        # Forward\n",
    "        y_pred, h = forward(x)\n",
    "        loss = cross_entropy(y_pred, y_true)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backprop\n",
    "        dy = y_pred.copy()\n",
    "        dy[y_true] -= 1  # dL/dy_pred\n",
    "\n",
    "        dW2 = np.outer(dy, h)\n",
    "        db2 = dy\n",
    "\n",
    "        dh = W2.T @ dy\n",
    "        dz1 = dh * (1 - h**2)  # tanh derivative\n",
    "\n",
    "        dW1 = np.outer(dz1, x)\n",
    "        db1 = dz1\n",
    "\n",
    "        # Update weights\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss/len(data):.4f}\")\n",
    "\n",
    "# --- Тест генерации ---\n",
    "ctx = 'a'\n",
    "for _ in range(5):\n",
    "    x = one_hot(token_to_idx[ctx], input_size)\n",
    "    y_pred, _ = forward(x)\n",
    "    idx = np.argmax(y_pred)\n",
    "    print(idx_to_token[idx], end=' ')\n",
    "    ctx = idx_to_token[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
