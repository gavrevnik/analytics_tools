{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Это краткий конспект полезных выводов из курса по нейросетям - https://habr.com/ru/articles/414165/\n",
    "# Также - полезная статья Яндекса со ссылками на источники - https://habr.com/ru/companies/yandex/articles/307260/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общая теория\n",
    "\n",
    "Задача ML = задача оптимизации весов модели на тренировочной выборке для лучшего предикта отложенной.  \n",
    "- Пусть у нас есть объекты $u_1, u_2, ...$ (например, клиенты)\n",
    "- Каждый объект $u_i$ может быть описан признаками в N-мерном пр-ве $x_{11}, x_{12}, ... x_{1N}$ \n",
    "- Мы решаем задачу классификации -> определяем какому из классов принадлежат объекты: $y_1, ... y_M$\n",
    "\n",
    "Есть тренировочный размеченный датасет Train = $u_1, u_2...$     \n",
    "Мы обучаем модель типа $y = Q(w_{1}, w_{2}, ... w_{K}, u_{1}, ... u_{N})$ с какими то весами  \n",
    "А далее делаем предсказание на отложенной выборке predict(Q, u_test) -> y_predict  \n",
    "\n",
    "\n",
    "___\n",
    "<u>Линейный классификатор</u>  \n",
    "y = (w, x) или в матричном виде: $W_{NM} * X_{N} = Y_{M}$  \n",
    "Здесь - $X_{N}$ столбец признаков объекта, $Y_{M}$ - столбец вероятностей принадлежности к каждому классу  \n",
    "\n",
    "___\n",
    "<u>Метод макс правдоподобия</u>  \n",
    "Макс. правдоподобие - общая метрика, которую надо оптимизировать для поиска корректных весов модели  \n",
    "\n",
    "Likelihood = p(y=y_train_1 | u_train_1) * ... * p(y=y_train_D | u_train_D)  \n",
    "Это вероятность того что мы получим верный $y_{train}$ из возможных $y_1, y_2 ... y_M$ для каждого объекта $u_{train}$  \n",
    "\n",
    "Вероятность получить $y_i$ для каждого u = x1, x2... xN задается через модель и ее веса **w**  \n",
    "\n",
    "Для оптимальных весов необходимо Likelihood -> MAX  \n",
    "Это эквивалентно -ln(Likelihood) -> MIN  \n",
    "Или Lossfunc = Loss $= -\\sum \\ln(p(y = y_{train} | u_{train})) -> min$ \n",
    "\n",
    "Для поиска минимума функции используется градиентный спуск\n",
    "\n",
    "___\n",
    "<u>Регуляризация</u>\n",
    "Для линейного классификатора можно написать LossFunc = L(w, x) -> grad(LossFunc, w)  \n",
    "Однако минимум Lossfunc может быть найден при разных значениях w (нет единственного решения).  \n",
    "Поэтому чтобы это исправить и норм оптимизировать - делают **регуляризацию**\n",
    "\n",
    "Lossfunc $= -\\sum \\ln(p) + \\lambda \\cdot R(w)$  \n",
    "\n",
    "Варианты R(w)  \n",
    "L1: R = |w1| + |w2| + ...  \n",
    "L2: $R^2 = w_1^2 + w_2^2 + ...$    \n",
    "\n",
    "L2 хорошо дифференциируется и помимо добавления однозначности - наказывает модель за очень большие $w_i$  \n",
    "Оказывается, плавное распределение значений весов $w_1, w_2 ...$ положительно сказывается на том, чтобы  \n",
    "модель не переобучалась (то есть не искала слишком частных решений которые фитят Train).  \n",
    "\n",
    "Итак, с помощью регуляризации получаем единственную точку для оптимизации, страхуемся от переобучения  \n",
    "lambda - фактор, задающий насколько сильно наказываем модель за избыточные веса. Подбирается как гиперпараметр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейном классификаторе мы по сути делаем следующие преобразования:  \n",
    "xv = (x1, ... xN)  ; $xv * w_{NM} = y_M$  \n",
    "\n",
    "Для нейросетей используем следующие цепочки преобразований:   \n",
    "tmp = xv * w_N_K1 # на выходе получим столбец величины K1  \n",
    "tmp = L(tmp) # применяем некоторое нелинейное преобразование (функция активации)  \n",
    "tmp = tmp * w_K1_K2 # очередное \"взвешивание\" матрицей - на выходе столбец K2  \n",
    "tmp = L(tmp)  \n",
    "...  \n",
    "tmp = tmp * w_Kx_M = y_M # здесь получаем столбец вероятностей принадлежности к классам  \n",
    "\n",
    "каждое умножение на матрицу w_x_y -> это новый **слой** обучаемой нейросети  \n",
    "Функция активации L(x) нелинейная, чтобы получаемая модель Q была более гибкой и настраиваемой.  \n",
    "Один из эффективных вариантов для обучения: L = Relu(x) = x if x > 0 else 0.  \n",
    "\n",
    "\n",
    "<u>Аналогия с нейронами</u>  \n",
    "Нейрон - это элемент принимающий на вход N сигналов типа x1, ... xN и выдающий M сигналов,  \n",
    "Линейный классификатор - это простейший нейрон  \n",
    "Каждый слой нейросети - матрица типа $W_{nm}$ <u>строка</u> которой является нейроном  \n",
    "Строка принимает на вход вектор объекта x1,...xN и возвращает y1,...yM\n",
    "\n",
    "___\n",
    "Мы получаем модель с рядом слоев, каждый из которых содержит K_x * K_y весов в общем случае.  \n",
    "Так как в каждом из слоев - линейное умножение, то можно аналитически записать LikeliHood модели.  \n",
    "Далее, также как и в другой модели - считаем $\\nabla Loss$, смещаем веса в направлении dw = $-\\nabla Loss$ и двигаемся  \n",
    "иттеративно пока не найдем минимум функции. Данные веса w_final - веса обученной нейросети\n",
    "\n",
    "При этом расчет градиентов на каждом шаге сводится к матричным операциям со слоями и хорошо параллелится  \n",
    "в плане вычислений. Поэтому обучение сетей производится на видеокартах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сверточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема классических сетей в огромном кол-ве обучаемых параметров  \n",
    "Каждый слой - это по сути матрица которая взвешивает каждый элемент  \n",
    "У одного объекта обучающей выборки u может быть широкое признаковое пространство N.   \n",
    "Например, если это картинка - то в ней может быть N>100k (rgb * число пикселей).  \n",
    "\n",
    "Принцип сверточных сетей - когда в каждом слое взвешивание (свертка) с весами идет по принципу  \n",
    "один нейрон слоя = некоторая область предудущего слоя (или объекта) - но не весь.  \n",
    "\n",
    "<img src=\"images/cnn.png\" width=\"500\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Промежуточные слои сверточной сети - это новые признаки вместо исходных признаков объектов.  \n",
    "Их можно рассматривать как более высокие уровни абстрации, генерируемые моделью в процессе обучения  \n",
    "так, чтобы уже на них осуществлять более эффективную классификацию.  \n",
    "\"Подсматривая\" в эти признаки, можно оценивать какие детали наиболее важны для модели\n",
    "\n",
    "Модель состоит из слоев типа convolution (свертка с обучаемым ядром), pooling (изменение размерности слоя),\n",
    "fully connected (слой, восстанавливающий размерность по числу классов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Текстовые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Общий пайплайн </u>\n",
    "1. Input (word vector)  \n",
    "2. Морфология (преобразования слов, падежи, окончания итд)\n",
    "3. Синтаксис (сборка слов в предложения, учет правил)\n",
    "4. Семантика (оценка \"настроения\" предложения, посыл итд)\n",
    "5. Контекст (перенос смыслов между несколькими предложениями)\n",
    "\n",
    "Имеем некоторый язык со всеми его словами (N ~ несколько млн)   \n",
    "Каждое слово здесь w = (0, 0, ... 1, 0, ... 0) - огромный вектор  \n",
    "\n",
    "Вектор слова w -> нейросеть -> не дискретный вектор wn = (0.1, -2, 3.2, ...)  \n",
    "Вектора wn - это абстрактное представление модели, в которое однозначно переводится вектор w  \n",
    "Вектор wn может быть меньшего размера, а также обладает свойствами семантической близости\n",
    "\n",
    "___\n",
    "<u> Обучение языковой модели - word2vec </u>  \n",
    "База - предсказание слова по входному контексту.  \n",
    "Мы даем модели на вход слова w1, w2, w3 ... wk -> она генерит вероятность встретить след. слово w_k+1  \n",
    "Можно например взять большой объем текста (все статьи Wiki) и идти по ним скользящим окном k, формируя Train  \n",
    "Точнее будет также брать слова до/после пропуска: \"Старик поймал __ рыбку\" (окно контекста)   \n",
    "В итоге для каждых двух слов мы можем оценить некоторую вероятность их совместного соседства.  \n",
    "Можно обучить модель так, чтобы итоговые вектора каждого слова были в таком пространстве, что  \n",
    "скалярное произведение этих векторов будет давать как раз вероятность соседства = близость слов.  \n",
    "При этом для модели - положительный факт встречи слов = текста, отрицательный = рандомные группы слов.  \n",
    "Неплохая статья - https://habr.com/ru/articles/446530/  \n",
    "Доп инфа по работе с языковыми моделями - https://web.stanford.edu/~jurafsky/slp3/3.pdf  \n",
    "Лекция по работе в w2v - https://www.youtube.com/watch?v=U0LOSHY7U5Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример обучения w2v модели на искусственном датасете. Для большей точности необходимо\n",
    "# импортировать датасеты с наборами слов извне\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample text data\n",
    "sentences = [\n",
    "    ['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    ['this', 'is', 'the', 'second', 'sentence'],\n",
    "    ['yet', 'another', 'sentence'],\n",
    "    ['one', 'more', 'sentence'],\n",
    "    ['and', 'the', 'final', 'sentence']\n",
    "]\n",
    "# Train Word2Vec model - \n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word_vector = model.wv['sentence']\n",
    "similar_words = model.wv.most_similar('sentence')\n",
    "print(\"Similar words to 'sentence':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные ньюансы обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "См подробнее в лекции - https://www.youtube.com/watch?v=OJlqsLg4E9Q\n",
    "\n",
    "<u> Проблема выбора функции активации </u>  \n",
    "Если выбрать функцию в стиле сигмоиды, то можно столкнуться с vanishing gradients -- градиенты  \n",
    "при решении оптимизационной задачи быстро будут зануляться и обучение будет останавливаться.  \n",
    "Функция Relu(x) лишена таких проблем\n",
    "\n",
    "<u> Проблема инициализации весов </u>  \n",
    "При случайной инициализации исходных весов модели w есть риск повышения нестабильности с каждым новым слоем.  \n",
    "Так как градиенты обучения будут пропорциональны весам и будут усиливать тенденцию к росту / спаду параметров.  \n",
    "Вариант - нормализировать веса после каждого слоя - приводить к распределению с тренируемыми параметрами.  \n",
    "В итоге модель сама подстроится - какую лучше пост-нормализацию после каждого слоя выбирать.  \n",
    "Это откроет путь к поиску существенно более стабильных конфигураций на базе большого кол-ва слоев.  \n",
    "Техника называется Batch normalization  \n",
    "\n",
    "То есть мы треним модель сразу на группе семплов - получаем от них статистику (среднее, дисперсия)  \n",
    "Используем их для пост-нормализации с подбираемыми параметрами avg, std  \n",
    "А при использовании обученной модели (фаза predict) -- подаем на вход один семпл  \n",
    "и используем полученные за тренировку значения статистики.  \n",
    "\n",
    "<u> Проблема переобучения </u>  \n",
    "Модель не должна наделять отдельные веса из группы весов исключительными свойствами -  \n",
    "это ведет к переобучению. Пример: если модель опознает по фото кошек, что это животное с хвостом  \n",
    "и фичу типа \"хвост\" определяет как ключевую - то легко может переобучиться. Нужно сказать ей что то  \n",
    "вроде \"смотри на хвост, но и на все другие признаки в совокупности\". То есть штрафовать за чрезмерное  \n",
    "внимание только к хвосту.  \n",
    "В линейном классификаторе (см выше) решали это регуляризацией - штрафом за величину отдельных весов.  \n",
    "В нейронной сети работает концепция <u> dropout </u> -- каждое обучение случайно дропаем часть (x%)  \n",
    "нейронов из каждого слоя. Это не дает завязаться на одном нейроне и перераспределяет веса на всю группу\n",
    "\n",
    "<img src=\"images/dropout.png\" width=\"400\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Проблема градиентного спуска </u>  \n",
    "Классический расчет градиента в некоторой точке M-мерной функции потерь - вычислительно долгая задача.  \n",
    "Порой используют стохастический спуск - случайный шаг в сторону -grad по любому из измерений. Больше не  \n",
    "совсем точных шагов, но зато быстрая сходимость глобально.  \n",
    "Также ускоряют алгоритмы подстраивая шаг оптимизации под крутизну спуска итд.  \n",
    "\n",
    "Когда M значительно - <u>редко</u> можно встретить локальные минимумы, но часто - седловые точки. Когда все  \n",
    "производные равны нулю, но вторые производные разных знаков. В этих точках спуск сильно буксует, прежде чем  \n",
    "найти в каком направлении спуститься с седла. Есть улучшения методов, которые ускоряют этот процесс.  \n",
    "\n",
    "Для реализации градиентного спуска в нейросетях используется метод back propagation.  \n",
    "Сначала задаем вектор весов w, далее рассчитываем на тренировочных данных значения на всех узлах сети.  \n",
    "Затем идем в обратном направлении, корректируя веса в узлах так, чтобы минимизировать отклонение от ожидаемого сигнала.   \n",
    "\n",
    "<u> Learning rate </u>  \n",
    "Эпоха (epoch) - иттерация в течение которой все данные Train пропущены через модель, определены градиенты весов w  \n",
    "Далее мы делаем изменение весов W_upd = W - learning_rate * grad(W). И идет следующая эпоха итд.  \n",
    "Величина learning_rate влияет на скорость обучения - не должна быть очень низкой или высокой.  \n",
    "Хорошая практика, когда сначала берем побольше, а каждые X эпох learning_rate уменьшается.\n",
    "\n",
    "<u> Оптимизация гиперпараметров </u>  \n",
    "Может быть много параметров (droout_percent, learning_rate, ...) которые надо подбирать для задачи.  \n",
    "Обычно выбирается 1 fold (train_date / validation_data) и на нем после train-обучения проверяется модель.  \n",
    "Можно делать grid-перебор гиперпараметров в поисках лучшего, либо random - чтобы в выборку попадало  \n",
    "больше семплов потенциально важных параметров.  \n",
    "\n",
    "Сначала перебор идет на макро масштабе, потом масштаб уточняется (по мере уточнения поиска).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Общая идея </u>  \n",
    "Обучаемая система - некоторый агент взаимодействующий с внешней средой, который совершает последовательно   действия (a=action) и на каждое действие получает от среды подкрепление (r=reward) - полож. или отрицательное.  \n",
    "Сессия взаимодействия агента со средой длится N иттераций (s = session). Агент учится так, чтобы  \n",
    "среднее подкрепление в течение сессии было максимальным.  \n",
    "\n",
    "Пример: обучение игре в аэрохоккей (коричневый скрин справа).  \n",
    "Передаем на вход агенту разницу previous/next изображений, ограничиваем взаимодействие N=50 шагами (сессия)  \n",
    "В конце сессии, если агент выиграл (шарик не упал) - дает reward>0 всем шагам. И наборот при поражении.  \n",
    "Прогоняем обучение много раз, чтобы агент делал действия (вверх/вниз) так, чтобы максимизировать reward за N=50  \n",
    "PS. reward за шаги может быть распределен неравномерно (затухать по мере увеличения N - для стабильности)  \n",
    "\n",
    "<img src=\"images/reinforcement_learning.png\" width=\"500\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рекуррентные сети (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классические нейронные сети имеют только один вход (один объект u) -> и несколько выходов (Y)  \n",
    "Хочется организовать возможность передавать на вход сети последовательность объектов u1, u2, ...   \n",
    "и получать ответы исходя из суммарного контекста.  \n",
    "К примеру, в задаче машинного перевода важно учитывать входную последовательность слов и их  \n",
    "порядок для генерации выходного перевода.  \n",
    "\n",
    "Это можно решить через RNN, когда каждый слой сети на вход получает инфу от своего элемента, а также предыдущего.  \n",
    "На скрине справа сеть тренируется предсказывать следующий символ по предыдущим.  \n",
    "В output она выдает следующую букву последовательности hello world\n",
    "\n",
    "Если, к примеру, обучить модель на Шекспире - то она будет пытаться говорить следующий символ (-> слово)  \n",
    "так, чтобы максимально попадать в текста Шекспира (то есть по сути впитает \"дух\" автора).  \n",
    "Теперь новые входные текста она будет дополнять в духе Шекспира.\n",
    "\n",
    "PS. Для обучения и расчета градиентов рекурентную сеть \"разматывают\" в линейную во времени  \n",
    "Это приводит к очень длинным цепочкам и большим вычислениям, поэтому есть архитектуры вроде LSTM,  \n",
    "которая помогает облегчить вычисления\n",
    "\n",
    "<img src=\"images/rnn.png\" width=\"350\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
